{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "3 - Building the SCI.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKHosiwbzXQO"
      },
      "source": [
        "### Building the SVM\n",
        "Here we will aim to represent the conversations using Bag-Of-Words (BOW) with a TF-IDF weighing scheme and then build our SVM Suspicious Conversations Identifier (SCI).\n",
        "\n",
        "First we read in the training data and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiYIZW-20DN2",
        "outputId": "0f5eb285-e10d-4059-df72-f11ed54df686"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4swqd-t7zXQ4"
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import csv\n",
        "\n",
        "\n",
        "def get_labels_dict(data_path):\n",
        "    labels_dict = {}\n",
        "    with open(data_path + 'sci_labels.csv', 'r') as f:\n",
        "        file = csv.reader(f)\n",
        "        for row in file:\n",
        "            labels_dict[row[0]] = row[1]\n",
        "    return labels_dict\n",
        "\n",
        "\n",
        "def get_features_labels(root, labels_dict):\n",
        "    corpus = [] # each row is a string formed from all messages in a conversations\n",
        "    labels = [] # each row is 0 or 1, corresponds to label for same row in corpus\n",
        "\n",
        "    for conversation in root:\n",
        "        string = \" \"\n",
        "        for message in conversation:\n",
        "            text = message.find('text').text\n",
        "            if text is not None:\n",
        "                string = string + \"\\r\\n\" + text \n",
        "        corpus.append(string)\n",
        "        labels.append(int(labels_dict[conversation.get('id')]))\n",
        "    return corpus, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULhdhCmuzXQ7"
      },
      "source": [
        "\n",
        "train_data_path = '/content/drive/MyDrive/online-grooming-detector-master/data/svm_training_data'\n",
        "training_xml = ET.parse(train_data_path + 'training_data.xml')\n",
        "train_root = training_xml.getroot()\n",
        "\n",
        "test_data_path = '/content/drive/MyDrive/online-grooming-detector-master/data/svm_test_data'\n",
        "test_data_src = '/content/drive/MyDrive/online-grooming-detector-master/data/pan12-sexual-predator-identification-training-corpus-2012-05-01'\n",
        "test_xml = ET.parse(test_data_src + 'pan12-sexual-predator-identification-test-corpus-2012-05-17.xml')\n",
        "test_root = test_xml.getroot()\n",
        "\n",
        "train_corpus, train_labels = get_features_labels(train_root, get_labels_dict(train_data_path))\n",
        "test_corpus, test_labels = get_features_labels(test_root, get_labels_dict(test_data_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy23H3w3zXQ9"
      },
      "source": [
        "We will now represent all conversations using BOW with TF-IDF weighing scheme.\n",
        "- [] Customize Vectorizer Parameters like normailize\n",
        "- [] Use hashing vectorization to save space and see if performance affected (https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLRzGWn-zXQ-"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import scipy\n",
        "# from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_corpus)\n",
        "X_test = vectorizer.transform(test_corpus)\n",
        "\n",
        "X_train = scipy.sparse.csr_matrix(X_train)\n",
        "y_train = np.array(train_labels)\n",
        "X_test = scipy.sparse.csr_matrix(X_test)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=87)\n",
        "# print(\"Train data shape:{}\\r\\nTest data shape:{}\".format(X_train.shape, X_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T-y4DgjzXQ_"
      },
      "source": [
        "We can now build the SVM and do cross validation to explore the accuracy of each kernel and hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw8ptafuzXQ_"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import heapq\n",
        "import operator\n",
        "import numpy as np\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "num_fold = 10\n",
        "k_fold = KFold(num_fold, True, 1)\n",
        "kernel = 'linear'\n",
        "acc = []\n",
        "\n",
        "for coef_c in np.arange(0.1, 5.1, 0.1):\n",
        "    acc_arr = np.zeros(num_fold)\n",
        "    ind = 0\n",
        "    for train_rows, val_rows in k_fold.split(X_train):\n",
        "#         model = svm.SVC(kernel=kernel, C=coef_c, gamma='auto', random_state=0)\n",
        "        model = LinearSVC(random_state=0, C=coef_c, loss='squared_hinge', dual=True)\n",
        "        model.fit(X_train[train_rows], y_train[train_rows])\n",
        "        pred_y = model.predict(X_train[val_rows])\n",
        "        acc_arr[ind] = metrics.accuracy_score(y_train[val_rows], pred_y)\n",
        "        ind += 1\n",
        "    acc.append([coef_c, np.mean(acc_arr)])\n",
        "    print(\"{}, c={}, Accuracy: {}\".format(kernel, coef_c, acc[len(acc)-1][1]))\n",
        "plt.plot([i[0] for i in acc], [i[1] for i in acc])\n",
        "plt.title(\"Performance of {} SVM\".format(kernel))\n",
        "plt.xlabel(\"C value\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "# plt.savefig('../output/As1_Qn4.2_' + kernel + '_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\") + '.png')\n",
        "plt.show()\n",
        "best = heapq.nlargest(1, acc, key=operator.itemgetter(1))[0]\n",
        "print(\"Best performing linear kernel SVM: C={}, Acc={}\".format(best[0], best[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-06RdxPmzXRF"
      },
      "source": [
        "Now that we have the best linear kernel model, let us test against our test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yroG_krozXRG"
      },
      "source": [
        "# model = svm.SVC(kernel='linear', C=best[0], gamma='auto', random_state=0)\n",
        "model = LinearSVC(random_state=0, C=best[0], loss='squared_hinge', dual=True)\n",
        "model.fit(X_train, y_train)\n",
        "pred_y = model.predict(X_test)\n",
        "print(metrics.accuracy_score(y_test, pred_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUL7hvP2zXRH"
      },
      "source": [
        "For linear kernel SVM with C coeficient of 1, we are getting an accuracy of 0.98561!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_4IlXeBzXRK"
      },
      "source": [
        "import pickle\n",
        "import datetime\n",
        "\n",
        "# save the model to the models folder\n",
        "filename = '../../models/SCI_SVM_' + \"{:.2f}_\".format(metrics.accuracy_score(y_test, pred_y)) + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\") + '.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}